
# Written by Dr Daniel Buscombe, Marda Science LLC
# for the USGS Coastal Change Hazards Program
#
# MIT License
#
# Copyright (c) 2021, Marda Science LLC
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import os, json, exifread, sys, getopt

USE_GPU = False #True

if USE_GPU == True:
   ##use the first available GPU
   os.environ['CUDA_VISIBLE_DEVICES'] = '0' #'1'
else:
   ## to use the CPU (not recommended):
   os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

#suppress tensorflow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow.keras.backend as K
from tensorflow.keras.models import model_from_json

from skimage.morphology import disk
from tkinter import filedialog
from tkinter import *
from skimage.io import imsave
from glob import glob
from joblib import Parallel, delayed
import numpy as np
import tensorflow as tf 
from skimage.morphology import remove_small_holes, remove_small_objects
from scipy.ndimage import maximum_filter
from skimage.transform import resize
from tqdm import tqdm
from skimage.filters import threshold_otsu
import matplotlib.pyplot as plt
from datetime import datetime

from skimage.exposure import adjust_log, adjust_gamma

SEED=42
np.random.seed(SEED)
AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API
tf.random.set_seed(SEED)

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))
#print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))


#-----------------------------------
def get_exif(f):
	"""
	read exif info from an image file
	"""
	# Open image file for reading (binary mode)
	f = open(f, 'rb')
	# Return Exif tags
	tags = exifread.process_file(f)
	f.close()

	exif = dict()
	for tag in tags.keys():
		if tag not in ('JPEGThumbnail', 'TIFFThumbnail', 'Filename', 'EXIF MakerNote'):
			#print("Key: %s, value %s" % (tag, tags[tag]))
			exif[tag]=tags[tag]
	return exif		

# #-----------------------------------
# legacy solution that doesnt work with joblib.Parallel
# import PIL.ExifTags
# import PIL.Image
# def get_exif_pil(f):
	# "only works in serial - why??"
	# img = PIL.Image.open(f)
	# exif_data = img.getexif()
	# exif = {
		# PIL.ExifTags.TAGS[k]: str(v)
		# for k, v in img._getexif().items()
		# if k in PIL.ExifTags.TAGS } #if v !=0
	# img.close()
	# return exif


##========================================================
def rescale(dat,
    mn,
    mx):
    '''
    rescales an input dat between mn and mx
    '''
    m = min(dat.flatten())
    M = max(dat.flatten())
    return (mx-mn)*(dat-m)/(M-m)+mn

##====================================
def standardize(img):
	"""
	image standardization using adjusted standard deviation
	"""
	N = np.shape(img)[0] * np.shape(img)[1]
	s = np.maximum(np.std(img), 1.0/np.sqrt(N))
	m = np.mean(img)
	img = (img - m) / s
	img = rescale(img, 0, 1)
	del m, s, N

	if np.ndim(img)!=3:
		img = np.dstack((img,img,img))

	return img

##==========================================================
def load_WM_json(weights_file, json_file):
	"""
	load keras Res-UNet model implementation from json file
	"""
	json_fp = open(json_file, 'r')
	loaded_model_json = json_fp.read()
	json_fp.close()
	model = model_from_json(loaded_model_json)

	model.load_weights(weights_file)
	model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
	return model

#-----------------------------------
def seg_file2tensor_3band(f, resize, TARGET_SIZE):
    """
    "seg_file2tensor(f)"
    This function reads a jpeg image from file into a cropped and resized tensor,
    for use in prediction with a trained segmentation model
    INPUTS:
        * f [string] file name of jpeg
    OPTIONAL INPUTS: None
    OUTPUTS:
        * image [tensor array]: unstandardized image
    GLOBAL INPUTS: TARGET_SIZE
    """
    bits = tf.io.read_file(f)
    if 'jpg' in f:
        bigimage = tf.image.decode_jpeg(bits)
    elif 'png' in f:
        bigimage = tf.image.decode_png(bits)

    if USE_LOCATION:
        gx,gy = np.meshgrid(np.arange(bigimage.shape[1]), np.arange(bigimage.shape[0]))
        loc = np.sqrt(gx**2 + gy**2)
        loc /= loc.max()
        loc = (255*loc).astype('uint8')
        bigimage = np.dstack((bigimage, loc))

    w = tf.shape(bigimage)[0]
    h = tf.shape(bigimage)[1]

    if resize:

        tw = TARGET_SIZE[0]
        th = TARGET_SIZE[1]
        resize_crit = (w * th) / (h * tw)
        image = tf.cond(resize_crit < 1,
                      lambda: tf.image.resize(bigimage, [w*tw/w, h*tw/w]), # if true
                      lambda: tf.image.resize(bigimage, [w*th/h, h*th/h])  # if false
                     )

        nw = tf.shape(image)[0]
        nh = tf.shape(image)[1]
        image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)

        return image, w, h, bigimage

    else:
        return None, w, h, bigimage


# =========================================================
def get_land_prob_n_var(E1, E2, W1, W2, w1, h1, w2, h2):
	"""
	resize all probability maps and average them =  output probability
	standard deviation in depthwise pixel stack is the variance map
	"""
	E1 = [resize(e,(w1,h1), preserve_range=True, clip=True) for e in E1]

	E2 = [resize(e,(w2,h2), preserve_range=True, clip=True) for e in E2]
	
	E = E1+E2
	W = W1+W2
	
	est_label = np.average(np.dstack(E), axis=-1, weights=np.array(W))
	est_label /= est_label.max()
	est_label[np.isnan(est_label)] = 0
	est_label[np.isinf(est_label)] = 0
	
	var = np.std(np.dstack(E), axis=-1)
	var[np.isnan(var)] = 0
	var[np.isinf(var)] = 0
	
	return est_label, var

# =========================================================
def get_confs(est_label, var, thres):
	"""
	average confidence is a score based on deviation from threshold
	average certainty is a weighted average of confidence, 1-variance, and absolute deviation from threshold 
	"""
	conf = 1-est_label
	conf[est_label<thres] = est_label[est_label<thres]
	conf = 1-conf
	conf[np.isnan(conf)] = 0
	conf[np.isinf(conf)] = 0
	
	model_conf = np.sum(conf)/np.prod(conf.shape)
	
	certainty = np.average(np.dstack((np.abs(est_label-thres) , conf , (1-var))), axis=2, weights=[2,1,1])
	
	certainty[np.isnan(certainty)] = 0
	certainty[np.isinf(certainty)] = 0
	return conf, model_conf, certainty
			
# =========================================================
def main(f, settings):
	"""
	main function that carries out all the computations on an individual image
	also writes outputs for a given input image
	"""
	try:
		exif = get_exif(f)
	except:
		print('no exif data in %s?'% (f))
		exif = dict()
	
	model1 = load_WM_json(settings['WEIGHTS1'], settings['MODEL_JSON1'])
	model2 = load_WM_json(settings['WEIGHTS2'], settings['MODEL_JSON2'])
	
	f2 = os.path.normpath(f)
	segfile = f2.replace(os.path.normpath(settings['SAMPLE_DIREC']), os.path.normpath(settings['SAMPLE_DIREC']+os.sep+'prob_stack')).replace('.jpg','.npz')

	# if os.path.exists(segfile):
		# print('%s exists ... skipping' % (segfile))
		# pass
	# else:
	
	try:
		if settings['verbose']==1:
			print('%s does not exist ... creating' % (segfile))

		#####=====================================
		## read files and standardized images
		
		image1, w1, h1, bigimage1 = seg_file2tensor_3band(f, True, settings['TARGET_SIZE1'])
		if image1 is None:
			image1 = bigimage1
		w1 = w1.numpy(); h1 = h1.numpy()

		image1 = standardize(image1.numpy()).squeeze()

		#image1 = adjust_log(image1,gain=1.2)
		image1 = adjust_gamma(image1,gamma=1/1.2) #.83 (brighter)

		image2, w2, h2, bigimage2 = seg_file2tensor_3band(f, True, settings['TARGET_SIZE2'])
		if image2 is None:
			image2 = bigimage2
		w2 = w2.numpy(); h2 = h2.numpy()

		if settings['verbose']==1:
			print("Working on %i x %i image" % (w2,h2))
		del bigimage2, bigimage1
		
		island_thres = 100*w1

		image2 = standardize(image2.numpy()).squeeze()

		image2 = adjust_log(image2,gain=1.2)

		#####=====================================
		## ensembling modeling
		
		E1 = []; W1 = []
		counter=0
		E1.append(model1.predict(tf.expand_dims(image1, 0) , batch_size=1).squeeze())
		W1.append(1)
		counter +=1
		E1.append(np.fliplr(model1.predict(tf.expand_dims(np.fliplr(image1), 0) , batch_size=1).squeeze()))
		W1.append(.5)
		counter +=1
		E1.append(np.flipud(model1.predict(tf.expand_dims(np.flipud(image1), 0) , batch_size=1).squeeze()))
		W1.append(.5)
		counter +=1

		K.clear_session()

		E2 = []; W2 = []
		counter=0
		E2.append(model2.predict(tf.expand_dims(image2, 0) , batch_size=1).squeeze())
		W2.append(1)
		counter +=1
		E2.append(np.fliplr(model2.predict(tf.expand_dims(np.fliplr(image2), 0) , batch_size=1).squeeze()))
		W2.append(.5)
		counter +=1
		E2.append(np.flipud(model2.predict(tf.expand_dims(np.flipud(image2), 0) , batch_size=1).squeeze()))
		W2.append(.5)
		counter +=1

		K.clear_session()

		if settings['extra_tta']==1:
			for k in np.linspace(100,int(TARGET_SIZE1[0]),10):
				E1.append(model.predict(tf.expand_dims(np.roll(image1, int(k)), 0) , batch_size=1).squeeze())
				W1.append(2*(1/np.sqrt(k)))
				counter +=1
			
			for k in np.linspace(100,int(TARGET_SIZE2[0]),10):
				E2.append(model.predict(tf.expand_dims(np.roll(image2, -int(k)), 0) , batch_size=1).squeeze())
				W2.append(2*(1/np.sqrt(k)))
				counter +=1
				K.clear_session()
		del image1, image2

		#####=====================================
		## probability map + variance map
		
		est_label, var = get_land_prob_n_var(E1, E2, W1, W2, w1, h1, w2, h2)
		
		del E1, E2, W1, W2

		meta = dict()
		meta['model1_target_size'] = settings['TARGET_SIZE1']
		meta['model2_target_size'] = settings['TARGET_SIZE2']		
		meta['model_version'] = settings['MODEL_VERSION']
		meta['num_tta'] = str(counter)
		meta['island_threshold' ] =island_thres
		meta['mean_var'] = str(np.mean(var))
		meta['median_var'] = str(np.median(var))
		meta['min_var'] =str( np.min(var))
		meta['max_var'] = str(np.max(var))
		meta['mean_prob_land'] = str(np.mean(est_label))
		meta['median_prob_land'] = str(np.median(est_label))
		meta['min_prob_land'] = str(np.min(est_label))
		meta['max_prob_land'] = str(np.max(est_label))
		meta['image_filename'] = f.split(os.sep)[-1]

		if settings['log'] >=1:
			logging.info("number of test-time-augmented outputs weighted-averaged %s" % (meta['num_tta']))
			logging.info("area in pixels^2 to be considered either a pixel island or hole %s" % (meta['island_threshold' ]))
			logging.info("mean variance in prediction %s" % (meta['mean_var'] ))
			logging.info("median variance in prediction %s" % (meta['median_var'] ))
			logging.info("min variance in prediction %s" % (meta['min_var'] ))
			logging.info("max variance in prediction %s" % (meta['max_var'] ))
			logging.info("mean probability of land %s" % (meta['mean_prob_land'] ))
			logging.info("median probability of land %s" % (meta['median_prob_land'] ))
			logging.info("min probability of land %s" % (meta['min_prob_land'] ))
			logging.info("max probability of land %s" % (meta['max_prob_land'] ))


		if np.max(est_label)-np.min(est_label) > .5:
			thres = threshold_otsu(est_label)
			if settings['verbose']==1:
				print("Probability of land threshold: %f" % (thres))
			meta['otsu_threshold_prob_land'] = str(thres)

			if settings['log'] >=1:
				logging.info("Otsu threshold for land %s" % (meta['otsu_threshold_prob_land']))
			
		else:
			thres = .9
			if settings['verbose']==1:
				print("Default threshold: %f" % (thres))
			meta['otsu_threshold_prob_land'] = str(0)

			if settings['log'] >=1:
				logging.info("Otsu threshold for land %s" % (meta['otsu_threshold_prob_land']))

		#####=====================================
		## confidence map + certainty map
		
		conf, model_conf, certainty =  get_confs(est_label, var, thres)
		
		if settings['verbose']==1:
			print('Overall model confidence = %f'%(model_conf))
		meta['mean_model_conf'] = str(model_conf)

		if settings['log'] >=1:
			logging.info("Mean model confidence %s" % (meta['mean_model_conf']))

		meta['mean_certainty'] = str(np.mean(certainty)	)
		
		if np.mean(certainty)<.5:
			output_code = 0
		elif (np.mean(certainty)>.5) and (np.mean(certainty)<.75):
			output_code=1
		else:
			output_code=2			

		thres_conf = threshold_otsu(conf)
		thres_var = threshold_otsu(var)
		if settings['verbose']==1:
			print("Confidence threshold: %f" % (thres_conf))
			print("Variance threshold: %f" % (thres_var))

		if settings['log'] >=1:
			logging.info("Mean model certainty %s" % (meta['mean_certainty']))

		#####=====================================
		## mask creation

		if settings['make_mask']==2:
			#mask1 (conservative)
			mask1 = (est_label>thres) & (conf>thres_conf) & (var<thres_var)
			mask1 = remove_small_holes(mask1.astype('bool'), island_thres)
			mask1 = remove_small_objects(mask1, island_thres).astype('uint8')

			outfile = segfile.replace('prob_stack', 'masks_'+str(settings['make_mask'])).replace('.npz','_mask.jpg')
			imsave(outfile,255*mask1.astype('uint8'),quality=100)

			if settings['log'] >=1:
				logging.info("%s-mask created for %s" % (f, settings['make_mask']))
				
		if settings['make_mask']==3:
			#mask2 
			mask2 = (est_label>thres) & (var<thres_var) 
			mask2 = remove_small_holes(mask2.astype('bool'), island_thres)
			mask2 = remove_small_objects(mask2, island_thres).astype('uint8')
			
			outfile = segfile.replace('prob_stack', 'masks_'+str(settings['make_mask'])).replace('.npz','_mask.jpg')
			imsave(outfile,255*mask2.astype('uint8'),quality=100)

			if settings['log'] >=1:
				logging.info("%s-mask created for %s" % (f, settings['make_mask']))
				
		if settings['make_mask']==4:
			#mask3
			mask3 = (est_label>thres) & (conf>thres_conf)
			mask3 = remove_small_holes(mask3.astype('bool'), island_thres)
			mask3 = remove_small_objects(mask3, island_thres).astype('uint8')

			outfile = segfile.replace('prob_stack', 'masks_'+str(settings['make_mask'])).replace('.npz','_mask.jpg')
			imsave(outfile,255*mask3.astype('uint8'),quality=100)

			if settings['log'] >=1:
				logging.info("%s-mask created for %s" % (f, settings['make_mask']))
				
		if settings['make_mask']==5:
			#mask4 (liberal)
			mask4 = (est_label>thres) 
			mask4 = remove_small_holes(mask4.astype('bool'), island_thres)
			mask4 = remove_small_objects(mask4, island_thres).astype('uint8')

			outfile = segfile.replace('prob_stack', 'masks_'+str(settings['make_mask'])).replace('.npz','_mask.jpg')
			imsave(outfile,255*mask4.astype('uint8'),quality=100)

			if settings['log'] >=1:
				logging.info("%s-mask created for %s" % (f, settings['make_mask']))
				
		if settings['make_mask']==1:
			#mask1 (conservative)
			mask1 = (est_label>thres) & (conf>thres_conf) & (var<thres_var)
			mask1 = remove_small_holes(mask1.astype('bool'), island_thres)
			mask1 = remove_small_objects(mask1, island_thres).astype('uint8')

			#mask2 
			mask2 = (est_label>thres) & (var<thres_var)
			mask2 = remove_small_holes(mask2.astype('bool'), island_thres)
			mask2 = remove_small_objects(mask2, island_thres).astype('uint8')

			#mask3
			mask3 = (est_label>thres) & (conf>thres_conf) 
			mask3 = remove_small_holes(mask3.astype('bool'), island_thres)
			mask3 = remove_small_objects(mask3, island_thres).astype('uint8')

			#mask4 (liberal)
			mask4 = (est_label>thres) 
			mask4 = remove_small_holes(mask4.astype('bool'), island_thres)
			mask4 = remove_small_objects(mask4, island_thres).astype('uint8')
			
			#mask0 (ultra conservative)
			mask0 = ((mask1+mask2+mask3+mask4)==4).astype('uint8')
			mask0 = remove_small_holes(mask0.astype('bool'), island_thres)
			mask0 = remove_small_objects(mask0, island_thres).astype('uint8')
			
			outfile = segfile.replace('prob_stack', 'masks_'+str(settings['make_mask'])).replace('.npz','_mask.jpg')
			imsave(outfile,255*mask0.astype('uint8'),quality=100)

			if settings['log'] >=1:
				logging.info("%s-mask created for %s" % (f, settings['make_mask']))

		if settings['make_mask']==6:
			#mask1 (conservative)
			mask1 = (est_label>thres) & (conf>thres_conf) & (var<thres_var)
			mask1 = remove_small_holes(mask1.astype('bool'), island_thres)
			mask1 = remove_small_objects(mask1, island_thres).astype('uint8')

			#mask2 
			mask2 = (est_label>thres) & (var<thres_var)
			mask2 = remove_small_holes(mask2.astype('bool'), island_thres)
			mask2 = remove_small_objects(mask2, island_thres).astype('uint8')

			#mask3
			mask3 = (est_label>thres) & (conf>thres_conf) 
			mask3 = remove_small_holes(mask3.astype('bool'), island_thres)
			mask3 = remove_small_objects(mask3, island_thres).astype('uint8')

			#mask4 (liberal)
			mask4 = (est_label>thres) 
			mask4 = remove_small_holes(mask4.astype('bool'), island_thres)
			mask4 = remove_small_objects(mask4, island_thres).astype('uint8')

			#mask5 (ultra liberal)
			mask5 = ((mask1+mask2+mask3+mask4)>=1).astype('uint8')	
			mask5 = remove_small_holes(mask5.astype('bool'), island_thres)
			mask5 = remove_small_objects(mask5, island_thres).astype('uint8')
			
			outfile = segfile.replace('prob_stack', 'masks_'+str(settings['make_mask'])).replace('.npz','_mask.jpg')
			imsave(outfile,255*mask5.astype('uint8'),quality=100)

			if settings['log'] >=1:
				logging.info("%s-mask created for %s" % (f, settings['make_mask']))
				

		if settings['all_masks']==1:
			if 'mask1' not in locals():
				mask1 = (est_label>thres) & (conf>thres_conf) & (var<thres_var)
				mask1 = remove_small_holes(mask1.astype('bool'), island_thres)
				mask1 = remove_small_objects(mask1, island_thres).astype('uint8')

			if 'mask2' not in locals():
				mask2 = (est_label>thres) & (var<thres_var)
				mask2 = remove_small_holes(mask2.astype('bool'), island_thres)
				mask2 = remove_small_objects(mask2, island_thres).astype('uint8')

			if 'mask3' not in locals():
				mask3 = (est_label>thres) & (conf>thres_conf) 
				mask3 = remove_small_holes(mask3.astype('bool'), island_thres)
				mask3 = remove_small_objects(mask3, island_thres).astype('uint8')

			if 'mask4' not in locals():
				mask4 = (est_label>thres) 
				mask4 = remove_small_holes(mask4.astype('bool'), island_thres)
				mask4 = remove_small_objects(mask4, island_thres).astype('uint8')

			if 'mask5' not in locals():
				mask5 = ((mask1+mask2+mask3+mask4)>=1).astype('uint8')			
				mask5 = remove_small_holes(mask5.astype('bool'), island_thres)
				mask5 = remove_small_objects(mask5, island_thres).astype('uint8')
			
			if 'mask0' not in locals():
				mask0 = ((mask1+mask2+mask3+mask4)==4).astype('uint8')
				mask0 = remove_small_holes(mask0.astype('bool'), island_thres)
				mask0 = remove_small_objects(mask0, island_thres).astype('uint8')
			
			M = [mask0, mask1, mask2, mask3, mask4, mask5]
			del mask0, mask1, mask2, mask3, mask4, mask5
				
		#####=====================================
		## make outputs
		
		meta['otsu_threshold_conf'] = str(thres_conf)
		meta['otsu_threshold_var'] = str(thres_var)
		meta['output_code'] = str(output_code)

		if settings['verbose']==1:
			logging.info("Otsu threshold confidence %s" % (meta['otsu_threshold_conf']))
			logging.info("Otsu threshold variance %s" % (meta['otsu_threshold_var']))
			logging.info("certainty output code (0=bad, 1=ok, 2=good) %s" % (meta['output_code']))

		meta['this_filename'] =segfile.split(os.sep)[-1]

		if settings['all_masks']==1:
			np.savez_compressed(segfile, 
						meta=meta, exif=exif, prob_land=(est_label*100).astype(np.uint8), conf=(conf*100).astype(np.uint8), 
						var=(var*100).astype(np.uint8), mask1 = M[0].astype('bool'), mask2 = M[1].astype('bool'),
						mask3 = M[2].astype('bool'), mask4 = M[3].astype('bool'),mask5 = M[4].astype('bool'),
						mask6 = M[5].astype('bool') ) 
			del meta, exif, est_label, conf, var, M

		else:
			np.savez_compressed(segfile, 
						meta=meta, exif=exif, prob_land=(est_label*100).astype(np.uint8), conf=(conf*100).astype(np.uint8), 
						var=(var*100).astype(np.uint8) ) 
			del meta, exif, est_label, conf, var

		if settings['log'] >=1:
			logging.info(datetime.now().strftime("%Y-%m-%d-%H-%M-%S"))
			logging.info("all outputs created for %s" % (f))
	except:
		print("image %s failed" % (f))
		pass
	
	
#====================================================
###==================================================================
#===============================================================
if __name__ == '__main__':

	argv = sys.argv[1:]
	try:
		opts, args = getopt.getopt(argv,"h:m:l:p:a:")
	except getopt.GetoptError:
		print('python watermask_images_in_folder.py -m mask mode -l make log file -p parallel processing\n -a store all masks')
		print('Defaults:\n')
		print('-m  = 0 (0=dont make a mask file, otherwise number indicates degree of liberalism; 1=most conservative, 6=most liberalk)\n')
		print('-l  = 1 (make log file, alt: 0=dont make log file)\n')
		print('-p  = 1 (use parallel proc, alt: 0=dont use parallel proc)\n')		
		print('-a  = 1 (make and store all 6 masks in npz file, alt: 0=dont store any)\n')				
		sys.exit(2)
	for opt, arg in opts:
		if opt == '-h':
			print('Example usage 1 [use all defaults]: python watermask_images_in_folder.py')
			print('Example usage 2 [no mask, no logfile]: python watermask_images_in_folder.py -m 0 -l 0') 
			print('Example usage 3 [liberal mask, logfile]: python watermask_images_in_folder.py -m 1 -l 1') 
			print('Example usage 4 [conservative mask, use default for logfile]: python watermask_images_in_folder.py -m 2') 
			print('Example usage 5 [disable parallel processing, otherwise use all defaults]: python watermask_images_in_folder.py -p 0') 			
			sys.exit()

		elif opt in ("-m"):
			make_mask = arg
			make_mask = int(make_mask)

		elif opt in ("-l"):
			log = arg
			log = int(log)

		elif opt in ("-p"):
			use_parallel = arg
			use_parallel = int(use_parallel)

		elif opt in ("-a"):
			all_masks = arg
			all_masks = int(all_masks)
			
	if 'make_mask' not in locals():
		make_mask = 0

	if 'log' not in locals():
		log = 1

	if 'use_parallel' not in locals():
		use_parallel = 1

	if 'all_masks' not in locals():
		all_masks = 1
		
	if log >=1:
		import logging
		try:
			os.mkdir(os.getcwd()+os.sep+'logs/')
		except:
			pass
		logging.basicConfig(filename=os.getcwd()+os.sep+'logs/'+datetime.now().strftime("%Y-%m-%d-%H-%M")+'.log',  level=logging.INFO) 
		logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

	if make_mask >6:
		mask_mask = 6

	if all_masks >1:
		all_masks = 1
		
	#====================================================
	print('........................................')
	print('ooooo...................................')
	print('xxxxoo..................................')
	print('xxxxxxo..............WEST COAST.........')
	print('xxxxxxxo................................')
	print('xxxxxxxxxxo.............................')
	print('xxxxxxxxxxxxxo..........................')
	print('xxxxxxxxxxxxxxxxxxxxo...................')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxo..............')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxo...........')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxo..........')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxo......')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxo.....')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxo...')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxo..')
	print('xxxxxxxxxxx WATERMASKER xxxxxxxxxxxxxxo.')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxo')
	print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

	from best_current_model import *

	WEIGHTS1 = os.path.normpath(WEIGHTS1)
	WEIGHTS2 = os.path.normpath(WEIGHTS2)

	configfile1 = WEIGHTS1.replace('.h5','.json').replace('weights', 'config')
	configfile2 = WEIGHTS2.replace('.h5','.json').replace('weights', 'config')

	MODEL_JSON1 = WEIGHTS1.replace('.h5','.json').replace("weights","models")
	MODEL_JSON2 = WEIGHTS2.replace('.h5','.json').replace("weights","models")

	with open(configfile1) as f:
		config1 = json.load(f)

	for k in config1.keys():
		exec(k+'=config1["'+k+'"]')

	TARGET_SIZE1 = TARGET_SIZE
	BATCH_SIZE1	= BATCH_SIZE

	with open(configfile2) as f:
		config2 = json.load(f)

	for k in config2.keys():
		exec(k+'=config2["'+k+'"]')	
		
	TARGET_SIZE2= TARGET_SIZE
	BATCH_SIZE2= BATCH_SIZE

	root = Tk()
	root.filename =  filedialog.askdirectory(initialdir = "/samples",title = "Select directory of images to segment")
	SAMPLE_DIREC = root.filename

	print('                                      ')
	print("Working on jpgs in %s" %  (SAMPLE_DIREC))
	root.withdraw()

	settings = dict()
	settings['WEIGHTS1'] = WEIGHTS1
	settings['WEIGHTS2'] = WEIGHTS2
	settings['MODEL_JSON1'] = MODEL_JSON1
	settings['MODEL_JSON2'] = MODEL_JSON2
	settings['TARGET_SIZE1'] = TARGET_SIZE1
	settings['TARGET_SIZE2'] = TARGET_SIZE2
	settings['BATCH_SIZE1'] = BATCH_SIZE1
	settings['BATCH_SIZE2'] = BATCH_SIZE2
	settings['MODEL_VERSION'] = MODEL_VERSION
	settings['SAMPLE_DIREC'] = SAMPLE_DIREC
	settings['make_mask'] = make_mask
	settings['log'] = log
	settings['all_masks'] = all_masks
	
	settings['extra_tta'] = 0
	settings['verbose']= 0

	if log ==1:
		logging.info(datetime.now().strftime("%Y-%m-%d-%H-%M-%S"))
		logging.info('input settings:')
		for k in settings.keys():
			logging.info(str(settings[k]))

	try:
		if settings['make_mask']>0:
			os.mkdir(SAMPLE_DIREC+os.sep+'masks_'+str(settings['make_mask']))
	except:
		pass

	try:
		os.mkdir(SAMPLE_DIREC+os.sep+'prob_stack')
	except:
		pass

	### predict
	print('.....................................')
	print('Using model for prediction on images ...')

	sample_filenames = sorted(glob(SAMPLE_DIREC+os.sep+'*.jpg'))

	if log ==1:
		logging.info(datetime.now().strftime("%Y-%m-%d-%H-%M-%S"))
		logging.info('sample files:')
		for k in sample_filenames:
			logging.info(k)
			
	print('Number of samples: %i' % (len(sample_filenames)))

	try:
		if use_parallel==1:
			w = Parallel(n_jobs=-2, verbose=0, timeout=1000, max_nbytes=None)\
					(delayed(main)(f, settings) \
					for f in tqdm(sample_filenames))
		else:
			for counter,f in enumerate(sample_filenames):
				main(f, settings)
				print('%i out of %i done'%(counter,len(sample_filenames)))		
	except:
		print("Something went wrong with parallel ... reverting to serial - WARNING: significantly slower")
		for counter,f in enumerate(sample_filenames):
			main(f, settings)
			print('%i out of %i done'%(counter,len(sample_filenames)))
			
		
## EOF



				# land_conservative = (est_label>thres) & (conf>thres_conf) & (var<thres_var)
			
		# land_conservative = remove_small_holes(land_conservative.astype('bool'), island_thres)
		# land_conservative = remove_small_objects(land_conservative, island_thres).astype('uint8')

		# meta['proportion_land_conservative'] = str(np.sum(land_conservative)/(w1*h1))
		# if settings['log'] >=1:
			# logging.info("Proportion of land pixels in conservative mask %s" % (meta['proportion_land_conservative']))
		
		# land_liberal = (est_label>thres) 
		# land_liberal = remove_small_holes(land_liberal.astype('bool'), island_thres)
		# land_liberal = remove_small_objects(land_liberal, island_thres).astype('uint8')
		# meta['proportion_land_liberal'] = str(np.sum(land_liberal)/(w1*h1))
		# if settings['log'] >=1:
			# logging.info("Proportion of land pixels in liberal mask %s" % (meta['proportion_land_liberal']))